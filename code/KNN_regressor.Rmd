---
title: "KNN regressor"
date: "`r Sys.Date()`"
output: pdf_document
toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)

```


In this report, I perform K Nearest Neighbour (KNN) model to predict saturation vapour pressure. KNN predicts the value of the target variable based on the average of the values of its k-nearest neighbors in the training set. Since KNN does not accept missing values in the dataset, I run the model on complete-case train data. 

# Initial KKN model with varying number of k


```{python}
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score, KFold


# read complete-case train data
train = pd.read_csv('../data/train.csv')

train = train.drop(columns='parentspecies')

X_train = train.iloc[:, 2:]
y_train = train['log_pSat_Pa']

mses = []
ks = range(1, X_train.shape[1] + 1)
for k in ks : 
  model = KNeighborsRegressor(n_neighbors=k) 
  kf = KFold(n_splits=10, shuffle=True, random_state=100) 
  fold_scores = -cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error') 
  mse = round(fold_scores.mean(), 3)
  mses.append(mse)

result = pd.DataFrame({'k': ks, 'MSE': mses}) 
print (result)    

# find the lowest MSE and MSE with 1% margin of error
min_mse = result['MSE'].min()
print(min_mse)
print(min_mse * 0.01 + min_mse)


```

Based on the table, the lowest MSE correspond to k=18 with MSE 5.167. Considering 1% margin of error, the optimal number of k is 10 with MSE 5.216.  

# KNN model improvement

## Features scaling

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() 
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns) 

mses = []
ks = range(1, X_train_scaled.shape[1] + 1)
for k in ks : 
  model = KNeighborsRegressor(n_neighbors=k) 
  kf = KFold(n_splits=10, shuffle=True, random_state=100) 
  fold_scores = -cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring='neg_mean_squared_error') 
  mse = round(fold_scores.mean(), 4)
  mses.append(mse)

result = pd.DataFrame({'k': ks, 'MSE': mses}) 
print (result)    

# find the lowest MSE and MSE with 1% margin of error
min_mse = result['MSE'].min()
print(min_mse)
print(min_mse * 0.01 + min_mse)


```
Based on the table, we see that scaled data achieves lower overall MSE. The lowest MSE which is 3.093 corresponds to k=17. Considering 1% margin of error, the optimal number of k is 10 with MSE 3.1210. 

The optimal number of k=10 aligns with the result obtained from unscaled data; however, the scaled data achieved a significantly lower MSE.


## Dimentionality reduction (PCA)

As the optimal number of k, achieved from both scaled and unscaled data is 10, I specify n_neighbors=10 in KNeighborsRegressor().

```{python}
from sklearn.decomposition import PCA

mses = []
for component in range(1, X_train_scaled.shape[1] + 1):
    pca = PCA(n_components=component)
    X_train_new = pca.fit_transform(X_train_scaled)

    model = KNeighborsRegressor(n_neighbors=10)  
    kf = KFold(n_splits=10, shuffle=True, random_state=100)
    fold_scores = -cross_val_score(model, X_train_new, y_train, cv=kf, scoring='neg_mean_squared_error')
    mse = round(fold_scores.mean(), 3)
    mses.append(mse)

result = pd.DataFrame({'PC': range(1, X_train_scaled.shape[1] + 1), 'MSE': mses})
print(result)


# find the lowest MSE and MSE with 1% margin of error
min_mse = result['MSE'].min()
print(min_mse)
print(min_mse * 0.01 + min_mse)

```

As indicated by the table, the dimensionality with minimum MSE is 9 with MSE 3.088. Considering 1% margin of error, the optimal dimensionality is 8 with MSE 3.108. 

In conclusion, we have achieved the best performance so far with the model using scaled data, k=10, and pc=8.

```{python}

train = train[['ID', 'log_pSat_Pa', 'MW', 'NumOfAtoms', 'NumOfO', 'NumOfC', 'NumOfConf', 'NumOfConfUsed', 'NumHBondDonors', 'hydroxyl (alkyl)', 'NumOfN', 'carboxylic acid', 'carbonylperoxynitrate', 'hydroperoxide', 'ketone', 'carbonylperoxyacid']]

X_train = train.iloc[:, 2:]
y_train = train['log_pSat_Pa']

X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns) 

mses = []
ks = range(1, X_train_scaled.shape[1] + 1)
for k in ks : 
  model = KNeighborsRegressor(n_neighbors=k) 
  kf = KFold(n_splits=10, shuffle=True, random_state=100) 
  fold_scores = -cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring='neg_mean_squared_error') 
  mse = round(fold_scores.mean(), 4)
  mses.append(mse)

result = pd.DataFrame({'k': ks, 'MSE': mses}) 
print (result)    

# find the lowest MSE and MSE with 1% margin of error
min_mse = result['MSE'].min()
print(min_mse)
print(min_mse * 0.01 + min_mse)

```
